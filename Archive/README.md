# Система индексации документов для EdTech сервиса

Система для создания и обновления векторной базы данных из документов, хранящихся в Cloudinary. Поддерживает обработку PDF, DOCX, MD и TXT файлов, извлечение текста и формул, а также создание векторных эмбеддингов для семантического поиска. Включает в себя LLM чат-бота для ответов на вопросы студентов.

## Особенности

- Поддержка форматов: PDF, DOCX, MD, TXT
- Извлечение текста и формул из документов
- Автоматическое разбиение текста на чанки
- Векторная база данных на основе FAISS
- Инкрементальное обновление базы данных
- Сохранение метаданных документов
- Поддержка LaTeX формул
- LLM чат-бот с использованием RAG для ответов на вопросы
- Механизм уточняющих вопросов для неясных запросов
- Сохранение истории диалогов

## Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd <repository-name>
```

2. Создайте виртуальное окружение и активируйте его:
```bash
python -m venv venv
source venv/bin/activate  # для Linux/Mac
# или
venv\Scripts\activate  # для Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Создайте файл .env на основе .env.example и заполните его своими данными:
```bash
cp .env.example .env
```

## Настройка

1. Получите учетные данные Cloudinary:
   - Зарегистрируйтесь на [Cloudinary](https://cloudinary.com/)
   - Перейдите в Dashboard
   - Скопируйте Cloud Name, API Key и API Secret

2. Заполните файл .env:
```
CLOUDINARY_CLOUD_NAME=your_cloud_name
CLOUDINARY_API_KEY=your_api_key
CLOUDINARY_API_SECRET=your_api_secret
```

## Использование

### Индексация документов

1. Запустите основной скрипт из корневой директории проекта:
```bash
python run.py
```

Скрипт выполнит следующие действия:
- Загрузит существующий индекс, если он есть
- Проверит наличие новых файлов в Cloudinary
- Обработает новые файлы и добавит их в векторную базу данных

### Чат-бот

1. Запустите пример использования чат-бота:
```bash
python examples/chat_bot_example.py
```

Чат-бот позволит:
- Задавать вопросы по учебным материалам
- Получать ответы с указанием источников
- Просматривать историю диалогов
- Сохранять историю диалогов

## Структура проекта

```
.
├── src/
│   ├── __init__.py         # Файл для обозначения директории как пакета Python
│   ├── cloudinary_client.py # Работа с Cloudinary
│   ├── document_processor.py # Обработка документов
│   ├── vector_store.py      # Векторная база данных
│   ├── document_indexer.py  # Основной класс системы
│   ├── chat_bot.py         # LLM чат-бот
│   ├── prompts.py          # Промпты для LLM
│   └── main.py             # Точка входа
├── examples/
│   └── chat_bot_example.py # Пример использования чат-бота
├── data/
│   └── vector_store/       # Директория для хранения индекса
├── requirements.txt
├── .env.example
├── run.py                  # Скрипт для запуска из корневой директории
└── README.md
```

## API

### DocumentIndexer

Основной класс для работы с системой индексации:

```python
from src.document_indexer import DocumentIndexer

# Инициализация
indexer = DocumentIndexer()

# Загрузка существующего индекса
indexer.load_existing_index()

# Обработка новых файлов
indexer.process_new_files()

# Поиск
results = indexer.search("ваш запрос", k=5)
```

### ChatBot

Класс для работы с чат-ботом:

```python
from src.chat_bot import ChatBot

# Инициализация
bot = ChatBot(
    model_name="mistralai/Mistral-7B-Instruct-v0.2",  # Можно заменить на другую модель
    vector_store_path="data/vector_store",
    enable_clarification=True
)

# Ответ на вопрос
response = bot.ask("ваш вопрос")

# Сохранение истории диалогов
bot.save_conversation_history("data/conversation_history.json")

# Загрузка истории диалогов
bot.load_conversation_history("data/conversation_history.json")
```

## Выбор модели для чат-бота

Для MVP с ограниченным бюджетом рекомендуется использовать **Mistral-7B-Instruct-v0.2**:

1. **Бесплатность**: Модель доступна бесплатно для исследовательских и коммерческих целей.
2. **Многоязычность**: Хорошо работает с русским языком.
3. **Качество ответов**: Достаточно мощная для генерации качественных ответов по различным дисциплинам.
4. **Локальный запуск**: Можно запускать локально, что снижает затраты и проблемы с приватностью.

Альтернативно, можно использовать **Llama 2** (7B или 13B параметров) или другие открытые модели.

## Лицензия

MIT 